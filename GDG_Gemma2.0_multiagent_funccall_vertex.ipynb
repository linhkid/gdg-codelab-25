{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcbg7NmEkIPi0HKITiJ1iN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhkid/gdg-codelab-25/blob/main/GDG_Gemma2.0_multiagent_funccall_vertex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codelab: Build your first agentic AI with Gemma 2.0 and Vertex AI"
      ],
      "metadata": {
        "id": "a7gUpn3TTtrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup and Authentication"
      ],
      "metadata": {
        "id": "PSl0HXChUPla"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9JbTdORvTMCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9040048-4f39-47cb-bffa-da975543dbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Required packages installed. Authenticating with Vertex AI...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies and authenticate with Vertex AI\n",
        "\n",
        "# @markdown This cell will install required packages and help you authenticate with Google Cloud.\n",
        "\n",
        "!pip install -q -U google-cloud-aiplatform\n",
        "!pip install -q matplotlib pandas numpy\n",
        "\n",
        "from google.colab import auth\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "# Authenticate to Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"‚úÖ Required packages installed. Authenticating with Vertex AI...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL_NAME = \"projects/991182027087/locations/asia-southeast1/models/gemma-2-2b-it-1742823574849\"\n",
        "MODEL_NAME = \"projects/991182027087/locations/asia-southeast1/endpoints/5762905479034437632\""
      ],
      "metadata": {
        "id": "zr53hp22ym81"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Initialize Vertex AI for Gemma 2.0"
      ],
      "metadata": {
        "id": "7KS6lKbjUT0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vertex AI and Gemma 2.0\n",
        "# @markdown Configure your project and set up Gemma 2.0 via Vertex AI\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
        "\n",
        "# Set your project ID\n",
        "PROJECT_ID = \"gdg-codelab-12thMay \"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    try:\n",
        "        # Try to retrieve project ID from environment variable\n",
        "        PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "        print(f\"Using project ID from environment: {PROJECT_ID}\")\n",
        "    except:\n",
        "        print(\"‚ùå Please set your Google Cloud project ID\")\n",
        "\n",
        "# Set location\n",
        "LOCATION = \"asia-southeast1\"  # Vertex AI region\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Gemma 2.0 model setup\n",
        "MODEL_NAME = \"projects/991182027087/locations/asia-southeast1/endpoints/5762905479034437632\"  # Instruction-tuned Gemma 2.0 model\n",
        "\n",
        "try:\n",
        "    # Load the Gemma 2.0 model from Vertex AI\n",
        "    model = GenerativeModel(MODEL_NAME)\n",
        "    print(f\"‚úÖ Successfully initialized {MODEL_NAME} on Vertex AI\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing model: {e}\")\n",
        "    print(\"Please check your project configuration and model availability in Vertex AI\")\n",
        "\n",
        "# Helper function for generating responses\n",
        "\n",
        "def generate_response(prompt, temperature=0.2, max_output_tokens=1024, top_p=0.8):\n",
        "    \"\"\"Generate a response from Gemma 2.0 on Vertex AI\"\"\"\n",
        "    request_json = {\n",
        "        \"instances\": [\n",
        "            {\n",
        "                \"inputs\": prompt\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    try:\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=temperature,\n",
        "            max_output_tokens=max_output_tokens,\n",
        "            top_p=top_p\n",
        "        )\n",
        "\n",
        "        response = model.generate_content(\n",
        "            json.dumps(request_json),\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        return \"Error generating response.\"\n",
        "\n",
        "# Helper function for displaying markdown format\n",
        "\n",
        "def display_markdown(text, render_markdown=True):\n",
        "    \"\"\"\n",
        "    Display text as Markdown in a Jupyter notebook.\n",
        "\n",
        "    Args:\n",
        "        text: The text to display (can contain Markdown formatting)\n",
        "        render_markdown: If True, renders the text as Markdown.\n",
        "                        If False, displays the raw Markdown source in a code block.\n",
        "\n",
        "    Returns:\n",
        "        None: Displays the formatted content in the notebook\n",
        "    \"\"\"\n",
        "    from IPython.display import display, Markdown, HTML\n",
        "\n",
        "    if render_markdown:\n",
        "        # Display text with Markdown rendering\n",
        "        display(Markdown(text))\n",
        "    else:\n",
        "        # Display raw Markdown source code in a code block\n",
        "        display(Markdown(f\"```markdown\\n{text}\\n```\"))\n"
      ],
      "metadata": {
        "id": "vKOrujaVUj26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85572773-f299-48bf-972e-068b6fa722e1"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully initialized projects/991182027087/locations/asia-southeast1/endpoints/5762905479034437632 on Vertex AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application 1: Structured Information Extraction"
      ],
      "metadata": {
        "id": "e39NwB54UpZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract structured data from text using Gemma 2.0 on Vertex AI\n",
        "\n",
        "def extract_structured_info(text, schema_description):\n",
        "    \"\"\"\n",
        "    Extract structured information from text based on a schema\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to extract information from\n",
        "        schema_description (str): Description of the schema to extract\n",
        "\n",
        "    Returns:\n",
        "        dict: Extracted structured information\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"I need to extract structured information from the following text.\n",
        "\n",
        "    Text: \"{text}\"\n",
        "\n",
        "    Please extract the following information:\n",
        "    {schema_description}\n",
        "\n",
        "    Return your answer as a markdown bullet points.\n",
        "    \"\"\"\n",
        "    response = generate_response(prompt, temperature=0.1)\n",
        "\n",
        "    # Extract JSON from response\n",
        "    return response\n",
        "\n",
        "# Example: Extract event details\n",
        "event_text = \"\"\"\n",
        "AISC 2025, organized by AITOMATIC and NIC, features a comprehensive agenda that includes a technical conference on March 12‚Äì13 at the National Convention Center in Hanoi, followed by a policy forum on March 14 at the NIC (Hoa Lac, Hanoi).\n",
        "Global figures‚Äîsuch as the Prime Minister of Vietnam, world-leading academics, and high-profile industry executives‚Äîwill share trends, research breakthroughs, and nationwide policy perspectives on the semiconductor and AI sectors.\n",
        "Additionally, an Executive Leadership Retreat is scheduled on March 15‚Äì16 in Da Nang, providing exclusive networking opportunities, bilateral meetings, and curated activities for senior leaders and decision-makers.\n",
        "\n",
        "Among the confirmed speakers and participants are experts from corporate giants like Honeywell, Intel, AMD, and NXP, alongside forward-thinking researchers from Google DeepMind, Stanford University, and KAIST. Their sessions will tackle a variety of topics‚Äîfrom edge AI and generative AI to advanced semiconductor manufacturing processes, materials innovation, and cross-border collaborations. Bringing together enterprises, policymakers, and the top academic and industry minds, AISC 2025 aims to underscore Vietnam‚Äôs growing importance in the global AI-semiconductor ecosystem while shaping a roadmap for sustainable development and leadership in these critical technologies.\n",
        "Whether you‚Äôre interested in technical deep dives, networking with global pioneers, or policy-level gatherings, AISC 2025 offers a well-rounded experience. Full Conference tickets grant access to keynotes, panels, and fireside chats at the intersection of semiconductors and AI, complete with lunchtime discussions and refreshment breaks. The Executive Experience package extends the event to an intimate weekend retreat in Da Nang, featuring private roundtables, exclusive receptions, and even leisure activities like world-class golf‚Äîa perfect blend of business and cultural exploration.\n",
        "In essence, AISC 2025 stands as a multi-faceted platform that draws together top government leaders, academic scholars, and corporate trailblazers in both AI and semiconductor technology. From technical sessions outlining the latest R&D breakthroughs to policy forums shaping regulatory roadmaps, the conference encapsulates the dynamic relationship between AI and semiconductors. Couple that with networking receptions, investment discussions, and a vibrant startup pavilion, and it‚Äôs clear that AISC 2025 is poised to mark a pivotal moment in Vietnam‚Äôs rise as a hub of global tech innovation.\n",
        "\"\"\"\n",
        "\n",
        "event_schema = \"\"\"\n",
        "- event_name: The name of the event\n",
        "- date: When the event will occur\n",
        "- location: Where the event will take place\n",
        "- organizer: Who is organizing the event\n",
        "- focus_areas: Technologies or topics covered\n",
        "- ticket_info: Pricing and registration details\n",
        "- attendees: Expected number or type of attendees\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìä Structured Information Extraction Example:\")\n",
        "print(\"Extracting event details using Gemma 2.0 on Vertex AI...\\n\")\n",
        "\n",
        "event_details = extract_structured_info(event_text, event_schema)\n",
        "\n",
        "print(\"Extracted Event Details:\")\n",
        "#print(json.dumps(event_details, indent=2))\n",
        "print(display_markdown(event_details))"
      ],
      "metadata": {
        "id": "vCqZFPVfUyoa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "eaf013c6-2df0-4a79-c9cf-407ed04dd30f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Structured Information Extraction Example:\n",
            "Extracting event details using Gemma 2.0 on Vertex AI...\n",
            "\n",
            "Extracted Event Details:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's the extracted information in markdown bullet points:\n\n- **event_name:** AISC 2025\n- **date:** March 12-13, March 14, March 15-16, 2025\n- **location:** National Convention Center in Hanoi, Vietnam; Da Nang, Vietnam\n- **organizer:** AITOMATIC and NIC\n- **focus_areas:** \n    - Semiconductor and AI sectors\n    - Edge AI\n    - Generative AI\n    - Advanced semiconductor manufacturing processes\n    - Materials innovation\n    - Cross-border collaborations\n- **ticket_info:** \n    - Full Conference tickets grant access to keynotes, panels, fireside chats, lunchtime discussions, and refreshment breaks.\n    - Executive Experience package includes an intimate weekend retreat with private roundtables, exclusive receptions, and leisure activities like world-class golf.\n- **attendees:** \n    - Global figures (Prime Minister of Vietnam, world-leading academics, high-profile industry executives)\n    - Experts from corporate giants (Honeywell, Intel, AMD, NXP)\n    - Researchers from Google DeepMind, Stanford University, and KAIST\n    - Senior leaders and decision-makers \n    - Startups \n    - Policymakers \n    - Government leaders \n    - Academic scholars \n    - Corporate trailblazers in AI and semiconductor technology \n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application 2: Multi-agent Research System powered by Gemma 2.0"
      ],
      "metadata": {
        "id": "QBEdmIV8U5rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Build a simple multi-agent research system using Gemma 2.0 on Vertex AI\n",
        "\n",
        "class ResearchAgent:\n",
        "    \"\"\"\n",
        "    A multi-agent system for research powered by Gemma 2.0 on Vertex AI\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, debug=False):\n",
        "        self.model = model\n",
        "        self.debug = debug\n",
        "        self.agents = {\n",
        "            \"planner\": self._planning_agent,\n",
        "            \"researcher\": self._research_agent,\n",
        "            \"analyzer\": self._analysis_agent,\n",
        "            \"reporter\": self._report_agent\n",
        "        }\n",
        "\n",
        "    def _planning_agent(self, query):\n",
        "        \"\"\"Plan the research strategy\"\"\"\n",
        "        prompt = f\"\"\"You are a research planning specialist.\n",
        "\n",
        "Given the research query: \"{query}\"\n",
        "\n",
        "Create a detailed research plan with the following components:\n",
        "1. Key questions to answer\n",
        "2. Necessary data points to collect\n",
        "3. Analysis methods to apply\n",
        "4. Structure for the final report\n",
        "\n",
        "Return a structured, step-by-step plan.\n",
        "\"\"\"\n",
        "        return generate_response(prompt, temperature=0.2)\n",
        "\n",
        "    def _research_agent(self, query, plan):\n",
        "        \"\"\"Gather information based on the research plan\"\"\"\n",
        "        prompt = f\"\"\"You are a thorough research specialist.\n",
        "\n",
        "Research query: \"{query}\"\n",
        "\n",
        "Research plan:\n",
        "{plan}\n",
        "\n",
        "Simulate the research data collection process and provide:\n",
        "1. Key facts and data points you would gather\n",
        "2. Sources you would consult (simulated)\n",
        "3. Any challenges in data collection\n",
        "4. Preliminary observations from the collected data\n",
        "\n",
        "Present this as a research notes document.\n",
        "\"\"\"\n",
        "        return generate_response(prompt, temperature=0.3)\n",
        "\n",
        "    def _analysis_agent(self, research_notes):\n",
        "        \"\"\"Analyze the gathered information\"\"\"\n",
        "        prompt = f\"\"\"You are a data analysis specialist.\n",
        "\n",
        "Research notes:\n",
        "{research_notes}\n",
        "\n",
        "Provide a comprehensive analysis including:\n",
        "1. Key patterns and trends\n",
        "2. Notable correlations\n",
        "3. Potential causalities\n",
        "4. Gaps requiring further research\n",
        "5. Statistical observations (simulated)\n",
        "\n",
        "Present this as an analytical summary.\n",
        "\"\"\"\n",
        "        return generate_response(prompt, temperature=0.2)\n",
        "\n",
        "    def _report_agent(self, query, plan, research_notes, analysis):\n",
        "        \"\"\"Generate a final research report\"\"\"\n",
        "        prompt = f\"\"\"You are a professional report writer.\n",
        "\n",
        "Original query: \"{query}\"\n",
        "\n",
        "Research plan:\n",
        "{plan}\n",
        "\n",
        "Research notes:\n",
        "{research_notes[:500]}... [notes truncated]\n",
        "\n",
        "Analysis:\n",
        "{analysis[:500]}... [analysis truncated]\n",
        "\n",
        "Create a well-structured, professional research report with:\n",
        "1. Executive summary\n",
        "2. Introduction and background\n",
        "3. Methodology\n",
        "4. Key findings\n",
        "5. Discussion of implications\n",
        "6. Conclusions and recommendations\n",
        "7. Appendix (simulated references)\n",
        "\n",
        "The report should be comprehensive yet concise.\n",
        "\"\"\"\n",
        "        return generate_response(prompt, temperature=0.2, max_output_tokens=2048)\n",
        "\n",
        "    def execute_research(self, query):\n",
        "        \"\"\"Execute the full research pipeline\"\"\"\n",
        "        if self.debug:\n",
        "            print(\"üìã Starting research process...\")\n",
        "\n",
        "        # Step 1: Planning\n",
        "        if self.debug:\n",
        "            print(\"üß© Planning research approach...\")\n",
        "        plan = self.agents[\"planner\"](query)\n",
        "        if self.debug:\n",
        "            print(\"‚úÖ Research plan created\")\n",
        "\n",
        "        # Step 2: Research\n",
        "        if self.debug:\n",
        "            print(\"üîç Gathering research data...\")\n",
        "        research_notes = self.agents[\"researcher\"](query, plan)\n",
        "        if self.debug:\n",
        "            print(\"‚úÖ Research data collected\")\n",
        "\n",
        "        # Step 3: Analysis\n",
        "        if self.debug:\n",
        "            print(\"üìä Analyzing research data...\")\n",
        "        analysis = self.agents[\"analyzer\"](research_notes)\n",
        "        if self.debug:\n",
        "            print(\"‚úÖ Analysis complete\")\n",
        "\n",
        "        # Step 4: Reporting\n",
        "        if self.debug:\n",
        "            print(\"üìù Generating final report...\")\n",
        "        report = self.agents[\"report_agent\"](query, plan, research_notes, analysis)\n",
        "        if self.debug:\n",
        "            print(\"‚úÖ Report generated\")\n",
        "\n",
        "        # Return the complete research package\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"plan\": plan,\n",
        "            \"research_notes\": research_notes,\n",
        "            \"analysis\": analysis,\n",
        "            \"report\": report\n",
        "        }\n",
        "\n",
        "# Create a research agent\n",
        "research_system = ResearchAgent(model, debug=True)\n",
        "\n",
        "# Execute a research task\n",
        "research_query = \"What are the current trends and challenges in EV charging infrastructure in smart cities?\"\n",
        "\n",
        "print(\"\\nüî¨ Multi-agent Research System Example:\")\n",
        "print(f\"Executing research on: '{research_query}'\\n\")\n",
        "\n",
        "research_results = research_system.execute_research(research_query)\n",
        "\n",
        "# Display the final report\n",
        "print(\"\\nüìë Final Research Report:\")\n",
        "display(Markdown(research_results[\"report\"]))"
      ],
      "metadata": {
        "id": "xb3n3TMuwcK2",
        "outputId": "f2f9e4f7-3c64-4bce-cc95-5a06b3743e3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî¨ Multi-agent Research System Example:\n",
            "Executing research on: 'What are the current trends and challenges in EV charging infrastructure in smart cities?'\n",
            "\n",
            "üìã Starting research process...\n",
            "üß© Planning research approach...\n",
            "‚úÖ Research plan created\n",
            "üîç Gathering research data...\n",
            "Error generating response: 'str' object has no attribute 'get'\n",
            "‚úÖ Research data collected\n",
            "üìä Analyzing research data...\n",
            "‚úÖ Analysis complete\n",
            "üìù Generating final report...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'report_agent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-fe5fe177d99f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Executing research on: '{research_query}'\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mresearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresearch_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_research\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresearch_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m# Display the final report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-fe5fe177d99f>\u001b[0m in \u001b[0;36mexecute_research\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìù Generating final report...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"report_agent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresearch_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Report generated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'report_agent'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Yk3T7GHwmMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}